<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: S3 | Scott Alan Henry]]></title>
  <link href="http://scottalan.github.io/blog/categories/s3/atom.xml" rel="self"/>
  <link href="http://scottalan.github.io/"/>
  <updated>2014-11-05T09:10:26-06:00</updated>
  <id>http://scottalan.github.io/</id>
  <author>
    <name><![CDATA[scottalan]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Amazon S3 Database Backups]]></title>
    <link href="http://scottalan.github.io/blog/2014/11/04/amazon-s3-database-backups/"/>
    <updated>2014-11-04T09:22:21-06:00</updated>
    <id>http://scottalan.github.io/blog/2014/11/04/amazon-s3-database-backups</id>
    <content type="html"><![CDATA[<h3>NOTE: Once this is done the first time you can just copy the script and s3/ to the new site directory and modify the files.</h3>

<p><code>
cd /opt
sudo wget https://github.com/s3tools/s3cmd/archive/master.zip
sudo unzip master.zip
cd s3cmd-master/
sudo python setup.py install
which s3cmd (/usr/local/bin/s3cmd)
</code></p>

<hr />

<h1>Just above Drupal root:</h1>

<p><code>
mkdir s3; cd s3;
nano s3/mysql.cnf
</code></p>

<h1>Add this to the mysql.cnf file:</h1>

<p><code>
[client]
user=‘DB_USER'
password=‘DB_PASSWORD'
</code></p>

<h1>Make a log directory:</h1>

<p><code>
mkdir log
</code></p>

<hr />

<h1>You may need the following:</h1>

<p><code>
sudo apt-get install python-dateutil python-magic
</code></p>

<h1>Now we can configure s3</h1>

<p><code>
sudo s3cmd —configure;
</code></p>

<p>(You need the Access Key and the Secret Key from s3.)
You should see output like: Configuration saved to &lsquo;/home/[USER]/.s3cfg’
Move the .s3cfg file to your s3 directory</p>

<h1>Add the script just above Drupal root:</h1>

<p><code>
nano sqlbackup-s3.sh;
</code></p>

<h1>Add the following contents and make changes to variables where necessary:</h1>

<p><div><script src='https://gist.github.com/b006a66bb4f3417285d1.js'></script>
<noscript><pre><code>#!/bin/bash

# Make directories: log and backup at the $DIR.
# Move the &#39;.s3cfg&#39; file to the $DIR as well.

# NOTE: CHANGE ALL VARIABLES TO MATCH YOUR NEEDS!

# The name of the s3 bucket on Amazon.
S3BUCKET=&quot;REPLACE_BUCKET_NAME&quot;

# The name of the database to backup.
DBNAME=&quot;REPLACE_DB_NAME&quot;

# The email address to send log file to.
EMAIL=&quot;REPLACE_EMAIL_ADDRESS&quot;

# The path to directory above Drupal root.
DIR=&quot;REPLACE/PATH/TO/DIR&quot;

# Local sql dump directory.
SQLDUMP=$DIR/sqldumps

# The name of the file to be created for the backup.
FILE=$DBNAME-`date &quot;+%Y%m%d-%H%M&quot;`.sql.gz

if [ ! -f $DIR/s3/log/mysqlback.log ]; then
  /usr/bin/touch $DIR/s3/log/mysqlback.log
  echo &quot;subject: S3 database backup&quot; &gt;&gt; $DIR/s3/log/mysqlback.log
fi

mysqldump --defaults-extra-file=$DIR/s3/mysql.cnf $DBNAME | gzip -9 &gt; $SQLDUMP/$FILE
/usr/local/bin/s3cmd --config $DIR/s3/.s3cfg put $SQLDUMP/$FILE s3://$S3BUCKET/ &gt;&gt; $DIR/s3/log/mysqlback.log
sleep 5

# This assumes there is a &#39;subject: My subject&#39; line in mysqlback.log. Will not
# send without it.
sendmail $EMAIL &lt; $DIR/s3/log/mysqlback.log</code></pre></noscript></div>
</p>

<h1>If sendmail doesn’t send, make sure the log file has a subject line. Should be added by the script:</h1>

<p>subject: S3 database backup</p>

<h1>Run the script and make sure the db uploaded to S3.</h1>

<h1>Now we can setup a cron job.</h1>

<p>crontab -e</p>

<h1>Add the following replacing the path to the script:</h1>

<p>&#8220;`</p>

<h1>Backup the database at 12am and 12pm everyday.</h1>

<p>0 0,12 * * * /PATH/TO/DIR/ABOVE/ROOT/sqlbackup-s3.sh
&#8220;`</p>
]]></content>
  </entry>
  
</feed>
